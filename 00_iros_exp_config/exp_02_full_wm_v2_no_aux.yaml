# @package _global_
# 实验 2: full wm (完整 World Model V2)
# 目的: 展示完整 WM 的性能提升
no_aux_loss: true

defaults:
  - /benchmark/nav/socialnav_v2: falcon_hm3d_task_for_train
  - /habitat_baselines: habitat_baselines_rl_config_base
  - /habitat/simulator/sim_sensors@habitat_baselines.eval.extra_sim_sensors.third_rgb_sensor: third_rgb_sensor
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_1: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_2: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_3: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_4: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_5: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_6: single_fixed
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_stop: discrete_stop
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_move_forward: discrete_move_forward
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_turn_left: discrete_turn_left
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_turn_right: discrete_turn_right
  - /habitat/task/actions@habitat.task.actions.agent_1_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_2_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_3_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_4_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_5_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_6_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/lab_sensors@habitat.task.lab_sensors.agent_0_pointgoal_with_gps_compass: pointgoal_with_gps_compass_sensor
  - /habitat/task/lab_sensors@habitat.task.lab_sensors.agent_0_human_state_goal: human_state_goal_sensor
  - _self_

habitat:
  dataset:
    split: train
  gym:
    obs_keys:
      - agent_0_articulated_agent_jaw_depth
      - agent_0_pointgoal_with_gps_compass
      - agent_0_localization_sensor
      - agent_0_human_num_sensor
      - agent_0_human_state_goal  # 融合传感器
      - agent_0_oracle_humanoid_future_trajectory
  task:
    actions:
      agent_0_discrete_stop:
        lin_speed: 0.0
        ang_speed: 0.0
      agent_0_discrete_move_forward:
        lin_speed: 25.0
        ang_speed: 0.0
        allow_dyn_slide: True
      agent_0_discrete_turn_left:
        lin_speed: 0.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_0_discrete_turn_right:
        lin_speed: 0.0
        ang_speed: -10.0
        allow_dyn_slide: True
      agent_1_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_2_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_3_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_4_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_5_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_6_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True

habitat_baselines:
  evaluate: False
  verbose: True
  trainer_name: "falcon_trainer"
  torch_gpu_id: 0
  tensorboard_dir: "experiments/exp_02_full_wm_v2_no_aux/tb"
  video_dir: "experiments/exp_02_full_wm_v2_no_aux/video"
  checkpoint_folder: "experiments/exp_02_full_wm_v2_no_aux/checkpoints"
  test_episode_count: -1
  num_environments: 28
  num_updates: -1
  total_num_steps: 15000000
  log_interval: 1
  num_checkpoints: -1 # use checkpoint_interval instead of percent-based saving
  checkpoint_interval: 20 # save ckpt every 20 updates
  force_torch_single_threaded: True
  load_resume_state_config: False

  evaluator:
    _target_: habitat_baselines.rl.ppo.falcon_evaluator.FALCONEvaluator

  eval:
    use_ckpt_config: False
    should_load_ckpt: True

  rl:
    preemption:
      save_resume_state_interval: 20
    agent:
      type: "MultiAgentAccessMgr"
      num_agent_types: 7
      num_active_agents_per_type: [1, 1, 1, 1, 1, 1, 1]
      num_pool_agents_per_type: [1, 1, 1, 1, 1, 1, 1]
      agent_sample_interval: 20
      force_partner_sample_idx: -1
    policy:
      agent_0:
        name: "SocialNavWMPolicyV2"  # 使用World Model Policy
    ppo:
      clip_param: 0.2
      ppo_epoch: 2
      num_mini_batch: 8
      value_loss_coef: 0.5
      entropy_coef: 0.01
      lr: 2.5e-4
      eps: 1e-5
      max_grad_norm: 0.2
      num_steps: 128
      use_gae: True
      gamma: 0.99
      tau: 0.95
      use_linear_clip_decay: False
      use_linear_lr_decay: False
      reward_window_size: 50
      use_normalized_advantage: False
      hidden_size: 512
      use_double_buffered_sampler: False

    ddppo:
      sync_frac: 0.6
      distrib_backend: NCCL
      pretrained_weights: ""
      pretrained: False
      pretrained_encoder: False
      train_encoder: True
      reset_critic: True
      backbone: resnet50
      rnn_type: LSTM
      num_recurrent_layers: 2
      # Exclude WM from DDP allreduce (WM trained by separate optimizer, PPO uses detached WM features)
      ddp_ignore_param_patterns:
        - ".net.world_model."

    auxiliary_losses: {}  # 无 aux：no_aux_loss: true，不使用任何辅助损失

  # ==================== World Model (WorldModelConfig) ====================
  world_model:
    enabled: True
    train_world_model: True
    fusion_mode: late

    # Encoder / 视觉 (使用 Falcon ResNet encoder，与 policy 共享相同架构)
    encoder_type: falcon
    falcon_backbone: resnet50
    falcon_baseplanes: 32
    falcon_ngroups: 16
    falcon_pretrained_path: "pretrained_model/pretrained_habitat3.pth"
    freeze_wm_encoder: True  # 冻结预训练 encoder，仅训练 RSSM + decoders
    mlp_units: 256

    # RSSM
    dyn_stoch: 30
    dyn_deter: 200
    dyn_hidden: 200
    dyn_discrete: 32

    # 通用
    act: SiLU
    norm: True
    num_humans: 6
    pred_horizon: 4
    use_goal_conditioning: True
    state_goal_dim: 8
    residual: True  # 残差预测: 网络输出 delta, 最终 = anchor_pos + delta

    # 训练策略
    wm_train_ratio: 1.0
    wm_warmup_updates: 2
    wm_grad_clip: 100.0

    # WM 优化器与 batch
    wm_lr: 3e-4
    opt_eps: 1e-5
    weight_decay: 0.0
    wm_batch_size: 16
    wm_sequence_length: 50
    wm_epochs_per_update: 15  # 每次 WM 更新训 15 个 batch

    # Replay buffer
    replay_buffer_size: 20000
    replay_buffer_warmup: 5000

    # Loss 权重
    depth_loss_scale: 10.0
    traj_loss_scale: 0.01
    reward_loss_scale: 1.0
    kl_loss_scale: 0.1
    kl_free_bits: 1.0
    kl_dyn_scale: 0.5
    kl_rep_scale: 0.1

    # 分布式
    ddp: True
