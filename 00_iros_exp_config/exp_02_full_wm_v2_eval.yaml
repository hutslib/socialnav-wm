# @package _global_
# 实验 2 评估配置: full wm V2 (完整 World Model V2)
# 用法:
#   python -u -m habitat_baselines.run \
#       --config-name=exp_02_full_wm_v2_eval.yaml \
#       habitat_baselines.eval_ckpt_path_dir=experiments/exp_02_full_wm_v2/checkpoints/ckpt.XXX.pth
#
# 或通过 eval_experiment.sh:
#   ./01_iros_exp_scripts/eval_experiment.sh exp_02_full_wm_v2

defaults:
  - /benchmark/nav/socialnav_v2: falcon_hm3d_task_for_train
  - /habitat_baselines: habitat_baselines_rl_config_base
  - /habitat/simulator/sim_sensors@habitat_baselines.eval.extra_sim_sensors.third_rgb_sensor: third_rgb_sensor
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_1: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_2: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_3: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_4: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_5: single_fixed
  - /habitat_baselines/rl/policy@habitat_baselines.rl.policy.agent_6: single_fixed
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_stop: discrete_stop
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_move_forward: discrete_move_forward
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_turn_left: discrete_turn_left
  - /habitat/task/actions@habitat.task.actions.agent_0_discrete_turn_right: discrete_turn_right
  - /habitat/task/actions@habitat.task.actions.agent_1_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_2_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_3_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_4_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_5_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/actions@habitat.task.actions.agent_6_oracle_nav_randcoord_action_obstacle: oracle_nav_action
  - /habitat/task/lab_sensors@habitat.task.lab_sensors.agent_0_pointgoal_with_gps_compass: pointgoal_with_gps_compass_sensor
  - /habitat/task/lab_sensors@habitat.task.lab_sensors.agent_0_human_state_goal: human_state_goal_sensor
  - base_eval_vis
  - _self_

habitat:
  dataset:
    split: val
  environment:
    iterator_options:
      shuffle: False
  gym:
    obs_keys:
      - agent_0_articulated_agent_jaw_depth
      - agent_0_pointgoal_with_gps_compass
      - agent_0_localization_sensor
      - agent_0_human_num_sensor
      - agent_0_human_state_goal
      - agent_0_oracle_humanoid_future_trajectory
  task:
    actions:
      agent_0_discrete_stop:
        lin_speed: 0.0
        ang_speed: 0.0
      agent_0_discrete_move_forward:
        lin_speed: 25.0
        ang_speed: 0.0
        allow_dyn_slide: True
      agent_0_discrete_turn_left:
        lin_speed: 0.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_0_discrete_turn_right:
        lin_speed: 0.0
        ang_speed: -10.0
        allow_dyn_slide: True
      agent_1_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_2_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_3_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_4_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_5_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True
      agent_6_oracle_nav_randcoord_action_obstacle:
        type: OracleNavRandCoordAction_Obstacle
        motion_control: human_joints
        lin_speed: 10.0
        ang_speed: 10.0
        allow_dyn_slide: True

habitat_baselines:
  evaluate: True
  verbose: True
  trainer_name: "falcon_trainer"
  torch_gpu_id: 0
  tensorboard_dir: "eval_experiments/exp_02_full_wm_v2/tb"
  video_dir: "eval_experiments/exp_02_full_wm_v2/video"
  video_fps: 10
  checkpoint_folder: "experiments/exp_02_full_wm_v2/checkpoints"
  eval_ckpt_path_dir: "experiments/exp_02_full_wm_v2/checkpoints"
  test_episode_count: 50
  num_environments: 1
  num_updates: -1
  total_num_steps: 15000000
  log_interval: 1
  num_checkpoints: 1
  force_torch_single_threaded: True
  load_resume_state_config: False

  evaluator:
    _target_: habitat_baselines.rl.ppo.falcon_evaluator.FALCONEvaluator

  eval:
    use_ckpt_config: False
    should_load_ckpt: True
    video_option: ["disk"]
    # 是否将每帧保存为图片 (与 video_option 独立，可单独开启)
    save_images: True
    # 图片保存目录，不设则使用 video_dir/images
    # image_dir: "eval_experiments/exp_02_full_wm_v2/images"

  rl:
    agent:
      type: "MultiAgentAccessMgr"
      num_agent_types: 7
      num_active_agents_per_type: [1, 1, 1, 1, 1, 1, 1]
      num_pool_agents_per_type: [1, 1, 1, 1, 1, 1, 1]
      agent_sample_interval: 20
      force_partner_sample_idx: -1
    policy:
      agent_0:
        name: "SocialNavWMPolicyV2"
    ppo:
      clip_param: 0.2
      ppo_epoch: 2
      num_mini_batch: 8
      value_loss_coef: 0.5
      entropy_coef: 0.01
      lr: 2.5e-4
      eps: 1e-5
      max_grad_norm: 0.2
      num_steps: 128
      use_gae: True
      gamma: 0.99
      tau: 0.95
      use_linear_clip_decay: False
      use_linear_lr_decay: False
      reward_window_size: 50
      use_normalized_advantage: False
      hidden_size: 512
      use_double_buffered_sampler: False

    ddppo:
      sync_frac: 0.6
      distrib_backend: NCCL
      pretrained_weights: ""
      pretrained: False
      pretrained_encoder: False
      train_encoder: True
      reset_critic: True
      backbone: resnet50
      rnn_type: LSTM
      num_recurrent_layers: 2

    auxiliary_losses:
      people_counting:
        max_human_num: 6
        loss_scale: 0.1
      guess_human_position:
        max_human_num: 6
        position_dim: 2
        loss_scale: 0.1
      future_trajectory_prediction:
        max_human_num: 6
        future_step: 4
        loss_scale: 0.1

  # ==================== World Model (eval: 不训练，仅推理) ====================
  world_model:
    enabled: True
    train_world_model: False
    fusion_mode: late

    encoder_type: falcon
    falcon_backbone: resnet50
    falcon_baseplanes: 32
    falcon_ngroups: 16
    falcon_pretrained_path: null
    freeze_wm_encoder: True
    mlp_units: 256

    dyn_stoch: 30
    dyn_deter: 200
    dyn_hidden: 200
    dyn_discrete: 32

    act: SiLU
    norm: True
    num_humans: 6
    pred_horizon: 4
    use_goal_conditioning: True
    state_goal_dim: 8
    residual: True

    wm_train_ratio: 0.0
    wm_warmup_updates: 0
    wm_grad_clip: 100.0

    wm_lr: 3e-4
    opt_eps: 1e-5
    weight_decay: 0.0
    wm_batch_size: 16
    wm_sequence_length: 50
    wm_epochs_per_update: 0

    replay_buffer_size: 20000
    replay_buffer_warmup: 0

    depth_loss_scale: 10.0
    traj_loss_scale: 0.01
    reward_loss_scale: 1.0
    kl_loss_scale: 0.1
    kl_free_bits: 1.0
    kl_dyn_scale: 0.5
    kl_rep_scale: 0.1

    ddp: False
